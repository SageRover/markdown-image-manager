#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿ç§»å’Œä¿®å¤ç°æœ‰å›¾ç‰‡æ˜ å°„è¡¨ä¸­çš„è·¯å¾„åˆ†éš”ç¬¦é—®é¢˜
"""

import json
import os
import re
import shutil
from datetime import datetime

def normalize_path(path):
    """ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦ï¼Œç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§"""
    if not path:
        return path
    
    # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
    normalized = path.replace('\\\\', '/').replace('\\', '/')
    
    # å»é™¤é‡å¤çš„åˆ†éš”ç¬¦
    normalized = re.sub(r'/+', '/', normalized)
    
    # å»é™¤æœ«å°¾çš„åˆ†éš”ç¬¦ï¼ˆé™¤éæ˜¯æ ¹ç›®å½•æˆ–Windowsé©±åŠ¨å™¨æ ¹ç›®å½•ï¼‰
    if len(normalized) > 1 and normalized.endswith('/'):
        # ä¿ç•™Windowsé©±åŠ¨å™¨æ ¹ç›®å½•çš„æ–œæ  (å¦‚ D:/)
        if not (len(normalized) >= 3 and normalized[1:3] == ':/'):
            normalized = normalized.rstrip('/')
    
    return normalized

def analyze_mapping_file(mapping_file_path):
    """åˆ†æç°æœ‰æ˜ å°„æ–‡ä»¶ä¸­çš„è·¯å¾„é—®é¢˜"""
    
    print("ğŸ” åˆ†æç°æœ‰å›¾ç‰‡æ˜ å°„è¡¨")
    print("=" * 50)
    
    if not os.path.exists(mapping_file_path):
        print(f"âŒ æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {mapping_file_path}")
        return None, None
    
    try:
        with open(mapping_file_path, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")
        return None, None
    
    print(f"ğŸ“Š æ˜ å°„è¡¨ç»Ÿè®¡:")
    print(f"  æ€»æ¡ç›®æ•°: {len(mapping_data)}")
    
    # åˆ†æè·¯å¾„é—®é¢˜
    problematic_paths = []
    normalized_mapping = {}
    duplicates = []
    
    for local_path, remote_url in mapping_data.items():
        # æ£€æŸ¥æ˜¯å¦æœ‰æ··åˆåˆ†éš”ç¬¦
        has_mixed_separators = ('\\' in local_path and '/' in local_path)
        
        if has_mixed_separators:
            problematic_paths.append({
                'original': local_path,
                'remote_url': remote_url,
                'issue': 'mixed_separators'
            })
        
        # è§„èŒƒåŒ–è·¯å¾„
        normalized_local = normalize_path(local_path)
        
        # æ£€æŸ¥è§„èŒƒåŒ–åæ˜¯å¦æœ‰é‡å¤
        if normalized_local in normalized_mapping:
            duplicates.append({
                'path1': normalized_mapping[normalized_local]['original'],
                'path2': local_path,
                'normalized': normalized_local,
                'url1': normalized_mapping[normalized_local]['remote_url'],
                'url2': remote_url
            })
        else:
            normalized_mapping[normalized_local] = {
                'original': local_path,
                'remote_url': remote_url
            }
    
    print(f"  æ··åˆåˆ†éš”ç¬¦è·¯å¾„: {len(problematic_paths)} ä¸ª")
    print(f"  è§„èŒƒåŒ–åé‡å¤: {len(duplicates)} ä¸ª")
    
    # æ˜¾ç¤ºé—®é¢˜è¯¦æƒ…
    if problematic_paths:
        print(f"\nğŸš¨ å‘ç°çš„é—®é¢˜è·¯å¾„:")
        for i, problem in enumerate(problematic_paths[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  {i}. {problem['original']}")
            print(f"     è§„èŒƒåŒ–å: {normalize_path(problem['original'])}")
        
        if len(problematic_paths) > 5:
            print(f"     ... è¿˜æœ‰ {len(problematic_paths) - 5} ä¸ªé—®é¢˜è·¯å¾„")
    
    if duplicates:
        print(f"\nğŸ”„ è§„èŒƒåŒ–åçš„é‡å¤é¡¹:")
        for i, dup in enumerate(duplicates[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"  {i}. è§„èŒƒåŒ–è·¯å¾„: {dup['normalized']}")
            print(f"     åŸè·¯å¾„1: {dup['path1']}")
            print(f"     åŸè·¯å¾„2: {dup['path2']}")
            print(f"     URLç›¸åŒ: {dup['url1'] == dup['url2']}")
        
        if len(duplicates) > 3:
            print(f"     ... è¿˜æœ‰ {len(duplicates) - 3} ä¸ªé‡å¤é¡¹")
    
    return mapping_data, {
        'problematic_paths': problematic_paths,
        'duplicates': duplicates,
        'normalized_mapping': normalized_mapping
    }

def create_migration_plan(analysis_result):
    """åˆ›å»ºè¿ç§»è®¡åˆ’"""
    
    print(f"\nğŸ“‹ åˆ›å»ºè¿ç§»è®¡åˆ’")
    print("=" * 50)
    
    if not analysis_result:
        print("âŒ æ— æ³•åˆ›å»ºè¿ç§»è®¡åˆ’ï¼Œåˆ†æç»“æœä¸ºç©º")
        return None
    
    problematic_paths = analysis_result['problematic_paths']
    duplicates = analysis_result['duplicates']
    normalized_mapping = analysis_result['normalized_mapping']
    
    migration_plan = {
        'backup_needed': True,
        'actions': []
    }
    
    # 1. è·¯å¾„è§„èŒƒåŒ–æ“ä½œ
    if problematic_paths:
        migration_plan['actions'].append({
            'type': 'normalize_paths',
            'description': f'è§„èŒƒåŒ– {len(problematic_paths)} ä¸ªæ··åˆåˆ†éš”ç¬¦è·¯å¾„',
            'items': problematic_paths
        })
    
    # 2. é‡å¤é¡¹å¤„ç†
    if duplicates:
        migration_plan['actions'].append({
            'type': 'handle_duplicates',
            'description': f'å¤„ç† {len(duplicates)} ä¸ªé‡å¤é¡¹',
            'items': duplicates,
            'strategy': 'keep_first'  # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤åç»­é‡å¤é¡¹
        })
    
    # 3. éªŒè¯æ“ä½œ
    migration_plan['actions'].append({
        'type': 'validate',
        'description': 'éªŒè¯è¿ç§»åçš„æ˜ å°„è¡¨å®Œæ•´æ€§'
    })
    
    print(f"ğŸ“ è¿ç§»è®¡åˆ’:")
    for i, action in enumerate(migration_plan['actions'], 1):
        print(f"  {i}. {action['description']}")
    
    return migration_plan

def execute_migration(mapping_file_path, original_mapping, migration_plan):
    """æ‰§è¡Œè¿ç§»æ“ä½œ"""
    
    print(f"\nğŸš€ æ‰§è¡Œè¿ç§»æ“ä½œ")
    print("=" * 50)
    
    if not migration_plan:
        print("âŒ æ²¡æœ‰è¿ç§»è®¡åˆ’ï¼Œè·³è¿‡è¿ç§»")
        return False
    
    # 1. åˆ›å»ºå¤‡ä»½
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = f"{mapping_file_path}.backup_{timestamp}"
    
    try:
        shutil.copy2(mapping_file_path, backup_file)
        print(f"âœ… å·²åˆ›å»ºå¤‡ä»½: {backup_file}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
        return False
    
    # 2. æ‰§è¡Œè¿ç§»æ“ä½œ
    new_mapping = {}
    migration_log = []
    
    for local_path, remote_url in original_mapping.items():
        normalized_local = normalize_path(local_path)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¤„ç†é‡å¤é¡¹ï¼‰
        if normalized_local in new_mapping:
            # é‡å¤é¡¹å¤„ç†ï¼šä¿ç•™ç¬¬ä¸€ä¸ªï¼Œè®°å½•å†²çª
            existing_url = new_mapping[normalized_local]
            if existing_url != remote_url:
                migration_log.append({
                    'type': 'duplicate_conflict',
                    'path': normalized_local,
                    'kept_url': existing_url,
                    'discarded_url': remote_url,
                    'original_path': local_path
                })
            else:
                migration_log.append({
                    'type': 'duplicate_same_url',
                    'path': normalized_local,
                    'url': remote_url,
                    'original_path': local_path
                })
        else:
            new_mapping[normalized_local] = remote_url
            
            # è®°å½•è·¯å¾„å˜åŒ–
            if normalized_local != local_path:
                migration_log.append({
                    'type': 'path_normalized',
                    'original': local_path,
                    'normalized': normalized_local,
                    'url': remote_url
                })
    
    # 3. ä¿å­˜æ–°çš„æ˜ å°„æ–‡ä»¶
    try:
        with open(mapping_file_path, 'w', encoding='utf-8') as f:
            json.dump(new_mapping, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ä¿å­˜æ–°çš„æ˜ å°„æ–‡ä»¶")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")
        # æ¢å¤å¤‡ä»½
        try:
            shutil.copy2(backup_file, mapping_file_path)
            print(f"âœ… å·²æ¢å¤å¤‡ä»½æ–‡ä»¶")
        except:
            pass
        return False
    
    # 4. ä¿å­˜è¿ç§»æ—¥å¿—
    log_file = f"{mapping_file_path}.migration_log_{timestamp}.json"
    try:
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': timestamp,
                'original_count': len(original_mapping),
                'new_count': len(new_mapping),
                'migration_log': migration_log
            }, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ä¿å­˜è¿ç§»æ—¥å¿—: {log_file}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿ç§»æ—¥å¿—å¤±è´¥: {e}")
    
    # 5. æ˜¾ç¤ºè¿ç§»ç»“æœ
    print(f"\nğŸ“Š è¿ç§»ç»“æœ:")
    print(f"  åŸå§‹æ¡ç›®: {len(original_mapping)}")
    print(f"  è¿ç§»åæ¡ç›®: {len(new_mapping)}")
    print(f"  è·¯å¾„è§„èŒƒåŒ–: {len([log for log in migration_log if log['type'] == 'path_normalized'])}")
    print(f"  é‡å¤é¡¹åˆå¹¶: {len([log for log in migration_log if log['type'].startswith('duplicate')])}")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ”§ å›¾ç‰‡æ˜ å°„è¡¨è¿ç§»å·¥å…·")
    print("=" * 60)
    
    # é»˜è®¤æ˜ å°„æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    mapping_file_path = "image_mapping.json"
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨å¸¸è§ä½ç½®æŸ¥æ‰¾
    if not os.path.exists(mapping_file_path):
        possible_paths = [
            "image_mapping.json",
            "./image_mapping.json",
            "../image_mapping.json",
            "D:/åšæœäº‘ç¬”è®°/Typoraäº‘ç¬”è®°/image_mapping.json"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                mapping_file_path = path
                break
        else:
            print(f"âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ˜ å°„æ–‡ä»¶")
            print(f"è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶ä¹‹ä¸€å­˜åœ¨:")
            for path in possible_paths:
                print(f"  - {path}")
            return
    
    print(f"ğŸ“ ä½¿ç”¨æ˜ å°„æ–‡ä»¶: {mapping_file_path}")
    
    # 1. åˆ†æç°æœ‰æ˜ å°„æ–‡ä»¶
    original_mapping, analysis_result = analyze_mapping_file(mapping_file_path)
    
    if not original_mapping:
        return
    
    # 2. åˆ›å»ºè¿ç§»è®¡åˆ’
    migration_plan = create_migration_plan(analysis_result)
    
    # 3. è¯¢é—®ç”¨æˆ·æ˜¯å¦æ‰§è¡Œè¿ç§»
    if migration_plan and migration_plan['actions']:
        print(f"\nâ“ æ˜¯å¦æ‰§è¡Œè¿ç§»æ“ä½œï¼Ÿ")
        print(f"  - å°†åˆ›å»ºå¤‡ä»½æ–‡ä»¶")
        print(f"  - è§„èŒƒåŒ–æ‰€æœ‰è·¯å¾„åˆ†éš”ç¬¦")
        print(f"  - å¤„ç†é‡å¤é¡¹")
        
        # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æœ‰ç”¨æˆ·äº¤äº’
        # ç°åœ¨æˆ‘ä»¬å‡è®¾ç”¨æˆ·åŒæ„è¿ç§»
        user_confirm = True  # input("è¯·è¾“å…¥ 'yes' ç¡®è®¤: ").lower() == 'yes'
        
        if user_confirm:
            # 4. æ‰§è¡Œè¿ç§»
            success = execute_migration(mapping_file_path, original_mapping, migration_plan)
            
            if success:
                print(f"\nğŸ‰ è¿ç§»å®Œæˆï¼")
                print(f"  å¤‡ä»½æ–‡ä»¶å·²ä¿å­˜")
                print(f"  æ˜ å°„è¡¨å·²æ›´æ–°")
                print(f"  å»ºè®®é‡æ–°æ‰«æä»¥éªŒè¯ç»“æœ")
            else:
                print(f"\nâŒ è¿ç§»å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        else:
            print(f"\nâ¸ï¸ ç”¨æˆ·å–æ¶ˆè¿ç§»æ“ä½œ")
    else:
        print(f"\nâœ… æ˜ å°„è¡¨æ— éœ€è¿ç§»ï¼Œæ‰€æœ‰è·¯å¾„å·²è§„èŒƒåŒ–")

if __name__ == "__main__":
    main()